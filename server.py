import uvicorn
import os
import asyncio
import argparse
import sys
from typing import Optional, List, Dict, Any
from fastapi import FastAPI, Query, HTTPException
from fastapi.responses import FileResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from dlsite_classification.extract.extract import ExtractFolder
import re

app = FastAPI(title="DLsite Collection Manager", version="1.0.0")

# Enable CORS for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:3001", "http://127.0.0.1:3001"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def get_data_path():
    """Áç≤ÂèñÊï∏ÊìöË∑ØÂæëÔºåÂÑ™ÂÖàÁ¥öÔºöÂëΩ‰ª§Ë°åÂèÉÊï∏ > Áí∞Â¢ÉËÆäÊï∏ > È†êË®≠ÂÄº"""
    # 1. Ê™¢Êü•ÂëΩ‰ª§Ë°åÂèÉÊï∏
    parser = argparse.ArgumentParser(description='DLsite Collection Manager API Server')
    parser.add_argument('--data-path', '-d', 
                       help='Path to DLsite collection data directory')
    parser.add_argument('--port', '-p', type=int, default=8001,
                       help='Port to run the server on (default: 8001)')
    parser.add_argument('--host', default="0.0.0.0",
                       help='Host to bind the server to (default: 0.0.0.0)')
    
    # Âè™Ëß£ÊûêÂ∑≤Áü•ÁöÑÂèÉÊï∏ÔºåÈÅøÂÖçËàá uvicorn ÁöÑÂèÉÊï∏Ë°ùÁ™Å
    args, unknown = parser.parse_known_args()
    
    if args.data_path:
        return args.data_path, args.port, args.host
    
    # 2. Ê™¢Êü•Áí∞Â¢ÉËÆäÊï∏
    env_path = os.getenv('DLSITE_DATA_PATH')
    if env_path:
        return env_path, args.port, args.host
    
    # 3. ‰ΩøÁî®È†êË®≠ÂÄº
    default_paths = [
        "./test_game_info",  # Ê∏¨Ë©¶Êï∏Êìö
        "/mnt/d/R18/DLsite", # ÂéüÂßãÈ†êË®≠Ë∑ØÂæë
        "./data",            # ÈÄöÁî®Êï∏ÊìöË∑ØÂæë
    ]
    
    for path in default_paths:
        if os.path.exists(path):
            return path, args.port, args.host
    
    # Â¶ÇÊûúÈÉΩ‰∏çÂ≠òÂú®Ôºå‰ΩøÁî®Á¨¨‰∏ÄÂÄã‰ΩúÁÇ∫È†êË®≠ÂÄº
    return default_paths[0], args.port, args.host

# Áç≤ÂèñÊï∏ÊìöË∑ØÂæëÂíå‰º∫ÊúçÂô®ÈÖçÁΩÆ
DATA_PATH, SERVER_PORT, SERVER_HOST = get_data_path()
extract_data = ExtractFolder(DATA_PATH)

print(f"üóÇÔ∏è  Data path: {DATA_PATH}")
print(f"üåê Server will run on: http://{SERVER_HOST}:{SERVER_PORT}")

# Ê®ôÊ∫ñÂåñÂíåÂéªÈáçÂáΩÊï∏
def normalize_and_deduplicate_list(items):
    """Ê®ôÊ∫ñÂåñÂíåÂéªÈáçÂàóË°®È†ÖÁõÆ"""
    if not items:
        return []
    
    normalized_items = set()
    for item in items:
        if item and isinstance(item, str):
            # ÁßªÈô§ÂâçÂæåÁ©∫Ê†ºÂíåÁâπÊÆäÂ≠óÁ¨¶
            cleaned = item.strip().strip('/')
            if cleaned:
                # ÁßªÈô§Â§öÈ§òÁöÑÁ©∫Ê†ºÂíåÊú´Â∞æÁ©∫Ê†º
                cleaned = re.sub(r'\s+', ' ', cleaned).strip()
                # ÁßªÈô§ÈáçË§áÁöÑÁ©∫Ê†ºÊ®ôË®òÔºàÂ¶Ç "„É≠„É™ " -> "„É≠„É™"Ôºâ
                if cleaned not in [existing.strip() for existing in normalized_items]:
                    normalized_items.add(cleaned)
    
    return sorted(list(normalized_items))

def smart_merge_similar_items(items):
    """Êô∫ËÉΩÂêà‰ΩµÁõ∏‰ººÈ†ÖÁõÆ"""
    if not items:
        return []
    
    merged = {}
    for item in items:
        # Ê®ôÊ∫ñÂåñÈ†ÖÁõÆ
        base_item = item.strip().rstrip(' ')
        
        # Ê™¢Êü•ÊòØÂê¶ÊúâÁõ∏‰ººÈ†ÖÁõÆÂ∑≤Â≠òÂú®
        found_similar = False
        for existing_key in list(merged.keys()):
            # Â¶ÇÊûúÂÆåÂÖ®ÂåπÈÖçÊàñÂè™Â∑ÆÁ©∫Ê†ºÔºåÂêà‰Ωµ
            if (base_item == existing_key or 
                base_item.replace(' ', '') == existing_key.replace(' ', '') or
                base_item == existing_key.rstrip(' ') or
                base_item.rstrip(' ') == existing_key):
                merged[existing_key] += 1
                found_similar = True
                break
        
        if not found_similar:
            merged[base_item] = 1
    
    return [{'name': k, 'count': v} for k, v in sorted(merged.items(), key=lambda x: x[1], reverse=True)]

def split_and_normalize_formats(format_string):
    """ÂàÜÂâ≤‰∏¶Ê®ôÊ∫ñÂåñ‰ΩúÂìÅÂΩ¢ÂºèÂ≠ó‰∏≤"""
    if not format_string or not isinstance(format_string, str):
        return []
    
    # ÁßªÈô§ÈñãÈ†≠ÁöÑ "/"
    cleaned = format_string.strip().lstrip('/')
    if not cleaned:
        return []
    
    # ÂàÜÂâ≤Â§öÂÄãÊ†ºÂºè (‰ª• / Êàñ , ÂàÜÈöî)
    parts = re.split(r'[/,]', cleaned)
    
    normalized_parts = []
    for part in parts:
        part = part.strip()
        if part and part not in ['', '/']:
            # Ê®ôÊ∫ñÂåñÂ∏∏Ë¶ãÊ†ºÂºè
            if 'Èü≥Â£∞„ÅÇ„Çä' in part and 'Èü≥Ê•Ω„ÅÇ„Çä' in part and 'ÂãïÁîª„ÅÇ„Çä' in part:
                base = part.replace('Èü≥Â£∞„ÅÇ„Çä', '').replace('Èü≥Ê•Ω„ÅÇ„Çä', '').replace('ÂãïÁîª„ÅÇ„Çä', '').strip()
                if base:
                    normalized_parts.append(base)
                    normalized_parts.append('Èü≥Â£∞„ÅÇ„Çä')
                    normalized_parts.append('Èü≥Ê•Ω„ÅÇ„Çä') 
                    normalized_parts.append('ÂãïÁîª„ÅÇ„Çä')
            elif 'Èü≥Â£∞„ÅÇ„Çä' in part and 'Èü≥Ê•Ω„ÅÇ„Çä' in part:
                base = part.replace('Èü≥Â£∞„ÅÇ„Çä', '').replace('Èü≥Ê•Ω„ÅÇ„Çä', '').strip()
                if base:
                    normalized_parts.append(base)
                    normalized_parts.append('Èü≥Â£∞„ÅÇ„Çä')
                    normalized_parts.append('Èü≥Ê•Ω„ÅÇ„Çä')
            elif 'Èü≥Â£∞„ÅÇ„Çä' in part and 'ÂãïÁîª„ÅÇ„Çä' in part:
                base = part.replace('Èü≥Â£∞„ÅÇ„Çä', '').replace('ÂãïÁîª„ÅÇ„Çä', '').strip()
                if base:
                    normalized_parts.append(base)
                    normalized_parts.append('Èü≥Â£∞„ÅÇ„Çä')
                    normalized_parts.append('ÂãïÁîª„ÅÇ„Çä')
            elif 'Èü≥Ê•Ω„ÅÇ„Çä' in part and 'ÂãïÁîª„ÅÇ„Çä' in part:
                base = part.replace('Èü≥Ê•Ω„ÅÇ„Çä', '').replace('ÂãïÁîª„ÅÇ„Çä', '').strip()
                if base:
                    normalized_parts.append(base)
                    normalized_parts.append('Èü≥Ê•Ω„ÅÇ„Çä')
                    normalized_parts.append('ÂãïÁîª„ÅÇ„Çä')
            elif 'Èü≥Â£∞„ÅÇ„Çä' in part:
                base = part.replace('Èü≥Â£∞„ÅÇ„Çä', '').strip()
                if base:
                    normalized_parts.append(base)
                    normalized_parts.append('Èü≥Â£∞„ÅÇ„Çä')
            elif 'Èü≥Ê•Ω„ÅÇ„Çä' in part:
                base = part.replace('Èü≥Ê•Ω„ÅÇ„Çä', '').strip()
                if base:
                    normalized_parts.append(base)
                    normalized_parts.append('Èü≥Ê•Ω„ÅÇ„Çä')
            elif 'ÂãïÁîª„ÅÇ„Çä' in part:
                base = part.replace('ÂãïÁîª„ÅÇ„Çä', '').strip()
                if base:
                    normalized_parts.append(base)
                    normalized_parts.append('ÂãïÁîª„ÅÇ„Çä')
            else:
                normalized_parts.append(part)
    
    return normalize_and_deduplicate_list(normalized_parts)

class WorkResponse(BaseModel):
    code: str
    title: str
    company: str
    genre: List[str]
    image_url: str
    sample_images: List[str]
    introduction: str
    sale_date: Optional[str]
    work_format: List[str]
    age_rating: List[str]
    file_size: List[str]
    my_rating: Optional[str] = None
    my_collection: Optional[str] = None
    my_collections: Optional[List[str]] = None

class CompanyResponse(BaseModel):
    name: str
    work_count: int
    works: List[WorkResponse]

class SearchResponse(BaseModel):
    total: int
    works: List[WorkResponse]
    companies: List[str]
    genres: List[str]

# Helper function to convert work data
def convert_work_to_response(work: Any, work_folder: str) -> WorkResponse:
    try:
        if not work.info or not work.info.tag:
            return None
        
        tag = work.info.tag
        
        # Extract title
        title = work.name
        if tag.title and isinstance(tag.title, dict):
            title = list(tag.title.keys())[0] if tag.title else work.name
        
        # Extract company
        company = "Unknown"
        if tag.company and isinstance(tag.company, dict):
            company = list(tag.company.keys())[0]
        
        # Extract genres and deduplicate
        genre = []
        if tag.genre and isinstance(tag.genre, dict):
            # Remove duplicates while preserving order and filtering empty values
            seen = set()
            for g in tag.genre.keys():
                if g and g.strip():  # Filter out empty or whitespace-only strings
                    g_cleaned = g.strip()
                    g_lower = g_cleaned.lower()
                    if g_lower not in seen:
                        seen.add(g_lower)
                        genre.append(g_cleaned)
        
        # Extract work format with smart splitting and normalization
        work_format = []
        if tag.work_format and isinstance(tag.work_format, dict):
            for format_str in tag.work_format.keys():
                work_format.extend(split_and_normalize_formats(format_str))
            work_format = normalize_and_deduplicate_list(work_format)
        
        # Extract age rating
        age_rating = []
        if tag.age and isinstance(tag.age, dict):
            age_rating = list(tag.age.keys())
        
        # Extract file size (keeping original for size info)
        file_size = []
        if tag.file_capacity and isinstance(tag.file_capacity, dict):
            file_size = normalize_and_deduplicate_list(list(tag.file_capacity.keys()))
        
        # Extract sale date
        sale_date = None
        if tag.sale_date and isinstance(tag.sale_date, dict):
            sale_date = list(tag.sale_date.keys())[0] if tag.sale_date else None
        
        # Extract user rating
        my_rating = None
        if hasattr(tag, 'my_rating') and tag.my_rating:
            my_rating = tag.my_rating
        
        # Extract user collection
        my_collection = None
        if hasattr(tag, 'my_collection') and tag.my_collection:
            my_collection = tag.my_collection
        
        # Extract user collections (multiple)
        my_collections = None
        if hasattr(tag, 'my_collections') and tag.my_collections:
            my_collections = tag.my_collections
        
        # Get main image
        main_image = None
        sample_images = []
        if work.info.images:
            for img in work.info.images:
                if "img_main" in img:
                    main_image = f"/image?path={os.path.join(work.info.path, img)}"
                elif "img_smp" in img:
                    sample_images.append(f"/image?path={os.path.join(work.info.path, img)}")
            
            if not main_image and work.info.images:
                main_image = f"/image?path={os.path.join(work.info.path, work.info.images[0])}"
        
        return WorkResponse(
            code=work.code,
            title=title,
            company=company,
            genre=genre,
            image_url=main_image or "",
            sample_images=sample_images,
            introduction=tag.introduction or "",
            sale_date=sale_date,
            work_format=work_format,
            age_rating=age_rating,
            file_size=file_size,
            my_rating=my_rating,
            my_collection=my_collection,
            my_collections=my_collections
        )
    except Exception as e:
        print(f"Error converting work {work.code}: {e}")
        return None

@app.get("/")
async def root():
    return {"message": "DLsite Collection Manager API"}

@app.get("/status")
async def status():
    """Check API status and data loading"""
    if not extract_data.classification_table:
        try:
            await extract_data.scan_file()
        except Exception as e:
            return {"status": "error", "message": f"Failed to scan files: {e}"}
    
    total_companies = len(extract_data.classification_table)
    total_works = sum(len(company.work_item) for company in extract_data.classification_table.values())
    
    return {
        "status": "ok",
        "total_companies": total_companies,
        "total_works": total_works,
        "data_path": DATA_PATH
    }

@app.get("/works", response_model=SearchResponse)
async def get_works(
    search: Optional[str] = Query(None, description="Search in title, company, genre"),
    company: Optional[str] = Query(None, description="Filter by company"),
    collection: Optional[str] = Query(None, description="Filter by collection"),
    rating: Optional[str] = Query(None, description="Filter by rating"),
    tags: Optional[str] = Query(None, description="Filter by tags (comma-separated)"),
    tag_mode: str = Query("OR", description="Tag filter mode: OR or AND"),
    work_format: Optional[str] = Query(None, description="Filter by work format"),
    file_format: Optional[str] = Query(None, description="Filter by file format"),
    sort: str = Query("title", description="Sort by: title, sale_date, company, rating, collection"),
    limit: int = Query(24, ge=1, le=100),
    offset: int = Query(0, ge=0)
):
    # Ensure data is scanned
    if not extract_data.classification_table:
        await extract_data.scan_file()
    
    all_works = []
    all_companies = set()
    all_genres = set()
    
    # Process all companies and works
    for company_folder, company_data in extract_data.classification_table.items():
        # Extract company name from the first work's company.tag instead of folder name
        company_display_name = company_folder
        if company_data.work_item:
            first_work = next(iter(company_data.work_item.values()))
            if first_work.info and first_work.info.tag and first_work.info.tag.company:
                company_name_from_tag = list(first_work.info.tag.company.keys())[0]
                company_display_name = company_name_from_tag
        
        all_companies.add(company_display_name)
        
        for work_folder, work in company_data.work_item.items():
            work_response = convert_work_to_response(work, work_folder)
            if work_response:
                # Fix company name display
                work_response.company = company_display_name
                all_works.append(work_response)
                # Add genres to global collection for statistics
                for genre in work_response.genre:
                    if genre and genre.strip():
                        all_genres.add(genre.strip())
    
    # Apply filters
    filtered_works = all_works
    
    # Text search
    if search:
        search_lower = search.lower()
        filtered_works = [
            work for work in filtered_works
            if (search_lower in work.title.lower() or
                search_lower in work.company.lower() or
                search_lower in work.code.lower() or
                search_lower in work.introduction.lower() or
                any(search_lower in genre.lower() for genre in work.genre))
        ]
    
    # Company filter
    if company:
        filtered_works = [work for work in filtered_works if company.lower() in work.company.lower()]
    
    # Collection filter
    if collection:
        filtered_works = [
            work for work in filtered_works 
            if (work.my_collection and collection.lower() in work.my_collection.lower()) or
               (hasattr(work, 'my_collections') and work.my_collections and 
                any(collection.lower() in c.lower() for c in work.my_collections))
        ]
    
    # Rating filter
    if rating:
        min_rating = int(rating)
        filtered_works = [work for work in filtered_works 
                         if work.my_rating and int(work.my_rating) >= min_rating]
    
    # Tag filter with OR/AND logic
    if tags:
        tag_list = [tag.strip().lower() for tag in tags.split(',')]
        if tag_mode.upper() == "AND":
            # AND mode: work must have ALL selected tags
            filtered_works = [
                work for work in filtered_works
                if all(any(tag in genre.lower() for genre in work.genre) for tag in tag_list)
            ]
        else:
            # OR mode: work must have ANY of the selected tags
            filtered_works = [
                work for work in filtered_works
                if any(any(tag in genre.lower() for genre in work.genre) for tag in tag_list)
            ]
    
    # Work format filter
    if work_format:
        filtered_works = [
            work for work in filtered_works
            if any(work_format.lower() in wf.lower() for wf in work.work_format)
        ]
    
    # File format filter 
    if file_format:
        filtered_works = [
            work for work in filtered_works
            if any(file_format.lower() in ff.lower() for ff in work.file_size)
        ]
    
    # Sorting
    if sort == "sale_date":
        filtered_works.sort(key=lambda x: x.sale_date or "", reverse=True)
    elif sort == "company":
        filtered_works.sort(key=lambda x: x.company.lower())
    elif sort == "rating":
        filtered_works.sort(key=lambda x: int(x.my_rating) if x.my_rating else 0, reverse=True)
    elif sort == "collection":
        filtered_works.sort(key=lambda x: x.my_collection or "")
    else:  # title
        filtered_works.sort(key=lambda x: x.title.lower())
    
    # Pagination
    total = len(filtered_works)
    paginated_works = filtered_works[offset:offset + limit]
    
    return SearchResponse(
        total=total,
        works=paginated_works,
        companies=sorted(list(all_companies)),
        genres=sorted(list(all_genres))
    )

@app.get("/companies", response_model=List[CompanyResponse])
async def get_companies():
    if not extract_data.classification_table:
        await extract_data.scan_file()
    
    companies = []
    for company_name, company_data in extract_data.classification_table.items():
        works = []
        for work_folder, work in company_data.work_item.items():
            work_response = convert_work_to_response(work, work_folder)
            if work_response:
                works.append(work_response)
        
        if works:
            # Get company display name from first work's tag
            company_display_name = company_name
            if works:
                company_display_name = works[0].company
            
            companies.append(CompanyResponse(
                name=company_display_name,
                work_count=len(works),
                works=works
            ))
    
    return sorted(companies, key=lambda x: x.work_count, reverse=True)

@app.get("/work/{code}")
async def get_work_detail(code: str):
    if not extract_data.classification_table:
        await extract_data.scan_file()
    
    for company_name, company_data in extract_data.classification_table.items():
        for work_folder, work in company_data.work_item.items():
            if work.code == code:
                work_response = convert_work_to_response(work, work_folder)
                if work_response:
                    # Add additional detail information
                    tag = work.info.tag
                    additional_info = {
                        "series": list(tag.series.keys()) if tag.series else [],
                        "voice_actor": list(tag.voice_actor.keys()) if tag.voice_actor else [],
                        "author": list(tag.author.keys()) if tag.author else [],
                        "illustration": list(tag.illustration.keys()) if tag.illustration else [],
                        "scenario": list(tag.scenario.keys()) if tag.scenario else [],
                        "music": list(tag.music.keys()) if tag.music else [],
                        "file_format": list(tag.file_format.keys()) if tag.file_format else [],
                        "operating_environment": list(tag.operating_environment.keys()) if tag.operating_environment else [],
                        "update_date": list(tag.update_date.keys()) if tag.update_date else [],
                        "star": tag.star if tag.star else None,
                        "pages": list(tag.pages.keys()) if tag.pages else [],
                        "event": list(tag.event.keys()) if tag.event else [],
                        "other": list(tag.other.keys()) if tag.other else [],
                        "languages": list(tag.languages.keys()) if tag.languages else [],
                    }
                    return {**work_response.dict(), **additional_info}
    
    raise HTTPException(status_code=404, detail="Work not found")

@app.get("/genres")
async def get_all_genres():
    if not extract_data.classification_table:
        await extract_data.scan_file()
    
    genre_counts = {}
    for company_name, company_data in extract_data.classification_table.items():
        for work_folder, work in company_data.work_item.items():
            if work.info and work.info.tag and work.info.tag.genre:
                for genre in work.info.tag.genre.keys():
                    if genre and genre.strip():
                        normalized_genre = genre.strip()
                        if normalized_genre in genre_counts:
                            genre_counts[normalized_genre] += 1
                        else:
                            genre_counts[normalized_genre] = 1
    
    # ÂâµÂª∫ÂåÖÂê´Ë®àÊï∏ÁöÑÊ®ôÁ±§ÂàóË°®ÔºåÊåâ‰ΩøÁî®È†ªÁéáÊéíÂ∫è
    genres_with_counts = [
        {"name": genre, "count": count}
        for genre, count in genre_counts.items()
        if count > 0  # Âè™ÂåÖÂê´ÊúâË®àÊï∏ÁöÑÊ®ôÁ±§
    ]
    
    # Êåâ‰ΩøÁî®È†ªÁéáÊéíÂ∫èÔºàÂæûÈ´òÂà∞‰ΩéÔºâ
    genres_with_counts.sort(key=lambda x: x["count"], reverse=True)
    
    return {"genres": genres_with_counts}

@app.get("/work-formats")
async def get_all_work_formats():
    if not extract_data.classification_table:
        await extract_data.scan_file()
    
    all_work_formats = set()
    for company_name, company_data in extract_data.classification_table.items():
        for work_folder, work in company_data.work_item.items():
            if work.info and work.info.tag and work.info.tag.work_format:
                for format_str in work.info.tag.work_format.keys():
                    normalized_formats = split_and_normalize_formats(format_str)
                    all_work_formats.update(normalized_formats)
    
    return {"work_formats": sorted(list(all_work_formats))}

@app.get("/file-formats")
async def get_all_file_formats():
    if not extract_data.classification_table:
        await extract_data.scan_file()
    
    all_file_formats = set()
    for company_name, company_data in extract_data.classification_table.items():
        for work_folder, work in company_data.work_item.items():
            if work.info and work.info.tag and work.info.tag.file_format:
                for format_str in work.info.tag.file_format.keys():
                    normalized_formats = split_and_normalize_formats(format_str)
                    all_file_formats.update(normalized_formats)
    
    return {"file_formats": sorted(list(all_file_formats))}

class UserDataRequest(BaseModel):
    rating: Optional[str] = None
    collection: Optional[str] = None
    collections: Optional[List[str]] = None

@app.post("/work/{code}/user-data")
async def update_user_data(code: str, user_data: UserDataRequest):
    """Update user's rating and collection for a work"""
    if not extract_data.classification_table:
        await extract_data.scan_file()
    
    # Find the work
    for company_name, company_data in extract_data.classification_table.items():
        for work_folder, work in company_data.work_item.items():
            if work.code == code:
                if not work.info or not work.info.path:
                    raise HTTPException(status_code=404, detail="Work info not found")
                
                info_path = work.info.path
                
                # Save rating
                if user_data.rating is not None:
                    rating_file = os.path.join(info_path, "my_rating.tag")
                    try:
                        with open(rating_file, 'w', encoding='utf-8') as f:
                            f.write(user_data.rating)
                        # Update in-memory data immediately without full rescan
                        if work.info.tag:
                            work.info.tag.my_rating = user_data.rating
                    except Exception as e:
                        raise HTTPException(status_code=500, detail=f"Failed to save rating: {e}")
                
                # Save single collection (backward compatibility)
                if user_data.collection is not None:
                    collection_file = os.path.join(info_path, "my_collection.tag")
                    try:
                        with open(collection_file, 'w', encoding='utf-8') as f:
                            f.write(user_data.collection)
                        # Update in-memory data immediately
                        if work.info.tag:
                            work.info.tag.my_collection = user_data.collection
                    except Exception as e:
                        raise HTTPException(status_code=500, detail=f"Failed to save collection: {e}")
                
                # Save multiple collections
                if user_data.collections is not None:
                    collections_file = os.path.join(info_path, "my_collections.tag")
                    try:
                        with open(collections_file, 'w', encoding='utf-8') as f:
                            for collection in user_data.collections:
                                f.write(f"{collection}\n")
                        # Update in-memory data immediately
                        if work.info.tag:
                            work.info.tag.my_collections = user_data.collections
                    except Exception as e:
                        raise HTTPException(status_code=500, detail=f"Failed to save collections: {e}")
                    
                    # Also update single collection for backward compatibility
                    if user_data.collections:
                        collection_file = os.path.join(info_path, "my_collection.tag")
                        try:
                            with open(collection_file, 'w', encoding='utf-8') as f:
                                f.write(user_data.collections[0])
                            # Update in-memory data immediately
                            if work.info.tag:
                                work.info.tag.my_collection = user_data.collections[0]
                        except Exception as e:
                            print(f"Warning: Failed to save backward compatible collection: {e}")
                
                # No need to refresh all data - we've updated in-memory
                # await extract_data.scan_file()
                
                return {"status": "success", "message": "User data updated"}
    
    raise HTTPException(status_code=404, detail="Work not found")

@app.get("/collections")
async def get_collections():
    """Get all unique collection categories"""
    if not extract_data.classification_table:
        await extract_data.scan_file()
    
    collections = set()
    for company_name, company_data in extract_data.classification_table.items():
        for work_folder, work in company_data.work_item.items():
            if work.info and work.info.tag:
                # Add single collection
                if work.info.tag.my_collection:
                    collections.add(work.info.tag.my_collection)
                # Add multiple collections
                if hasattr(work.info.tag, 'my_collections') and work.info.tag.my_collections:
                    for collection in work.info.tag.my_collections:
                        collections.add(collection)
    
    return {"collections": sorted(list(collections))}

@app.get("/image")
async def get_image(path: str):
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Image not found")
    return FileResponse(path)

@app.get("/scan")
async def scan():
    use_time = await extract_data.scan_file()
    return {"status": "ok", "scan_time": use_time, "total_works": len([
        work for company in extract_data.classification_table.values()
        for work in company.work_item.values()
    ])}

if __name__ == "__main__":
    # Ê™¢Êü•Êï∏ÊìöË∑ØÂæëÊòØÂê¶Â≠òÂú®
    if not os.path.exists(DATA_PATH):
        print(f"‚ö†Ô∏è  Warning: Data path '{DATA_PATH}' does not exist!")
        print(f"üìÅ You can specify a custom path using:")
        print(f"   python server.py --data-path /path/to/your/data")
        print(f"   or set environment variable: export DLSITE_DATA_PATH=/path/to/your/data")
        print(f"üöÄ Server will start anyway, but scanning will fail until the path exists.")
        print()
    
    uvicorn.run(
        "server:app",
        host=SERVER_HOST,
        port=SERVER_PORT,
        log_level="info"
    )
